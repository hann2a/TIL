대분류,중분류,소분류/내용,출처
AI 파운데이션 모델 개념 및 대표 모델,AI 파운데이션 모델의 개념,파운데이션 모델(Foundation Model)은 대규모 데이터를 폭넓게 학습한 후, 다양한 문제에 빠르게 적용할 수 있는 범용 대형 AI 모델이다.,
AI 파운데이션 모델 개념 및 대표 모델,AI 파운데이션 모델의 개념,파운데이션 모델은 함수 또는 프로그램이며, 학습 때 보지 못했던 데이터에 대해서도 작동해야 하는 의무가 있다.,
AI 파운데이션 모델 개념 및 대표 모델,AI 파운데이션 모델의 특징,특징 1 [대규모]: 트랜스포머 모델과 대규모 언어 학습을 사용하며, 주로 쉬운 데이터 수집과 대규모 학습을 기반으로 하는 비지도 학습 모델이 등장한다.,
AI 파운데이션 모델 개념 및 대표 모델,AI 파운데이션 모델의 특징,특징 2 [적응성]: 높은 파인튜닝 성능(높은 태스크 적응 성능)을 가지며, 믿고 쓸 수 있는 모델이다.,
AI 파운데이션 모델 개념 및 대표 모델,AI 파운데이션 모델의 특징,특징 3 [범용성]: 다양한 작업과 한정되지 않는 출력(예: 만 개 이상의 물체 분류)을 지원한다.,
AI 파운데이션 모델 개념 및 대표 모델,AI 모델 개발의 변화 (활용 기법),파운데이션 모델은 사전 학습(Pre-training) 후 Zero-shot/Few-shot 사용이나 Fine-tuning(미세 조정)을 통해 활용된다.,
AI 파운데이션 모델 개념 및 대표 모델,대표적 모델: CLIP (2021) by OpenAI,CLIP은 AI가 언어와 시각을 통합해서 이해하는 방식을 보여주는 패러다임 전환을 제시했다 (Contrastive Language-Image Pre-training).,
AI 파운데이션 모델 개념 및 대표 모델,대표적 모델: CLIP (2021) by OpenAI,CLIP의 학습 방식은 대조 학습(Contrastive Pre-training) 기반의 언어-이미지 사전 학습이며, 인터넷의 이미지-자연어 쌍을 사용한다.,
AI 파운데이션 모델 개념 및 대표 모델,대표적 모델: CLIP (2021) by OpenAI,CLIP 모델은 텍스트 인코더 (Transformer 기반)와 이미지 인코더 (ViT 기반)로 구성되어 있다.,
AI 파운데이션 모델 개념 및 대표 모델,대표적 모델: CLIP (2021) by OpenAI,CLIP 학습은 목표 이미지(앵커)를 대응하는 텍스트(양성)와 가깝게, 일치하지 않는 텍스트(음성)와는 멀게 임베딩 공간에서 학습하는 것을 목표로 한다.,
AI 파운데이션 모델 개념 및 대표 모델,대표적 모델: CLIP (2021) by OpenAI,CLIP의 응용 사례로는 제로샷 이미지 인식이 있으며, 텍스트 기반 카테고리 리스트를 텍스트 임베딩으로 변환하여 쿼리 이미지와 비교한다.,
Vision-Language Model (VLM),AGI를 향해서,대규모 언어 모델(LLM)이 높은 사고/추론 능력을 보인 후, 현실 세계 이해를 위해 시각 능력이 보강된 Vision-Language Model(VLM)이 중요해지고 있다.,
Vision-Language Model (VLM),시각언어 모델 예시 (GPT-4),GPT-4 (2023)는 이미지, 문서, 음성 등 멀티모달 데이터를 처리할 수 있는 모델로 발전했다.,
Vision-Language Model (VLM),시각언어 모델 예시 (GPT-4),GPT-4는 손글씨 스케치를 웹페이지(HTML) 코드로 변환하는 등의 멀티모달 능력을 보여준다.,
Vision-Language Model (VLM),멀티모달 정합 응용,멀티모달 정합(Multi-modal Alignment)은 서로 다른 두 가지 이상의 모달리티(예: 이미지와 텍스트) 간의 공동 임베딩 벡터 공간을 구성하여 유사도를 비교할 수 있게 한다.,
Vision-Language Model (VLM),고도화된 CLIP 계열 모델,SigLIP (2023)은 기존 CLIP의 대조 학습 한계를 개선하기 위해 softmax 대신 sigmoid 기반 손실 함수를 사용하며, CLIP 대비 우수한 성능을 보인다.,
Vision-Language Model (VLM),멀티모달 언어 모델 (LLM과 결합),LLaVA (2023)는 Vision과 Language 모델을 결합한 VLM으로, 이미지 인식 및 텍스트 생성, 시각적 질문 응답(Visual QA) 등에 사용된다.,
Vision-Language Model (VLM),멀티모달 언어 모델 (LLM과 결합),Qwen-VL (Alibaba)은 상용 VLM에 맞는 오픈소스 VLM으로, 여러 개의 이미지 입력, 번역, 텍스트 읽기, 물체 탐지, 인식 능력 등을 제공한다.,
Vision-Language Model (VLM),멀티모달 언어 모델 (LLM과 결합),InternVL (2024)은 LLM의 대규모 용량에 맞추어 이미지 인코더의 용량을 증대시킨 오픈소스 VLM이다.,
Vision-Language Model (VLM),도메인 특화 파운데이션 모델,의료 도메인 특화 모델(예: LLaVA-Med)은 의료 이미지를 포함한 지시문 데이터 학습을 통해 의료 이미지 기반 챗봇 대화가 가능하다.,
Vision-Language Model (VLM),도메인 특화 파운데이션 모델,제조업 도메인 특화 모델(AnomalyGPT)은 이미지 상의 결함을 탐지하고 챗봇 형식으로 질의응답을 제공한다.,
Vision-Language Model (VLM),도메인 특화 파운데이션 모델,로봇 행동 모델(PaLM-E)은 텍스트 명령과 로봇 시점 영상을 입력받아 로봇 행동을 출력하는 다중 모달 모델이다.,
Small VLM과 파운데이션 모델들 소개,sVLM (Small VLM) 개발 시도,대규모 VLM의 높은 비용과 리소스 문제를 해결하기 위해 개인 컴퓨터나 스마트폰에서 실행 가능한 경량화된 VLM(sVLM) 개발이 시도되고 있다.,
Small VLM과 파운데이션 모델들 소개,sVLM 모델 예시,SmolVLM은 Huggingface가 개발했으며, LLM 부분을 경량화하여 GPU RAM 사용량을 줄였다.,
Small VLM과 파운데이션 모델들 소개,sVLM 모델 예시,Moondream 0.5B는 모바일/엣지 디바이스의 실시간 실행을 목표로 개발되었으며, 2억 개의 파라미터로 구성되어 있다.,
Small VLM과 파운데이션 모델들 소개,sVLM 모델 예시,Gemini Nano는 온디바이스용 경량 Gemini 모델로, 스마트폰 등 디바이스 내부에서 직접 실행되도록 설계되었다.,
Small VLM과 파운데이션 모델들 소개,한국어 sVLM의 필요성,언어별 구조적/형태적 차이 때문에 영어 중심 토크나이저 사용 시 토큰 낭비(구조적 불이익)가 발생하므로 한국어 특화 모델이 필요하다.,
Small VLM과 파운데이션 모델들 소개,한국어 sVLM 모델,HyperCLOVAX-SEED-Vision-Instruct-3B (NAVER)는 한국어 특화 멀티모달 모델이다.,
Small VLM과 파운데이션 모델들 소개,이미지 세그멘테이션 모델,Segment Anything (SAM)은 범용 객체 분할 모델로, 클릭, 박스, 텍스트 등 유저의 입력을 받아 원하는 영역 마스크를 추출한다.,
Small VLM과 파운데이션 모델들 소개,이미지 세그멘테이션 모델,Grounding SAM은 Grounding DINO와 SAM을 결합하여 텍스트 입력으로 객체 탐지와 분할을 동시에 수행할 수 있다.,
Small VLM과 파운데이션 모델들 소개,이미지 생성 모델,DALL·E 3 (OpenAI)는 대화를 통해 프롬프트 개선 및 이미지를 생성할 수 있으며 ChatGPT 서비스에 내장되어 있다.,
Small VLM과 파운데이션 모델들 소개,이미지 생성 모델,ControlNet은 컨트롤 조건 입력을 기반으로 사용자가 원하는 이미지를 생성하는 모델이며, 파인튜닝을 통해 용도 변경이 가능하다.,
Small VLM과 파운데이션 모델들 소개,비디오 생성 모델,Sora (OpenAI, 2024)는 텍스트를 입력으로 받아 최대 1080p, 20초 길이의 비디오를 생성할 수 있으며, 물리적 이해를 보여주는 월드 모델의 가능성을 내포한다.,
Small VLM과 파운데이션 모델들 소개,비디오 생성 모델,Veo 3 (Google Gemini)은 자연스럽게 싱크된 소리까지 함께 생성하는 비디오 생성 모델이다.,
개인화, 합성 데이터 활용 사례,파운데이션 모델 + Fine-tuning,파운데이션 모델은 범용적이지만, 특정 작업이나 도메인에 최적화되어 있지 않기 때문에 미세 조정(Fine-tuning)이 필요하다.,
개인화, 합성 데이터 활용 사례,Fine-tuning의 정의 및 장점,Fine-tuning은 이미 학습된 모델을 추가 학습을 통해 조금만 튜닝하는 것으로, 적은 데이터와 리소스로 특정 작업에 우수한 성능을 확보할 수 있다.,
개인화, 합성 데이터 활용 사례,Parameter-Efficient Fine-Tuning (PEFT),PEFT는 높은 비용 문제를 해결하기 위한 효율적인 미세 조정 방법으로, 프롬프트 튜닝(Prompt Tuning)과 Adaptor 모델 추가 학습 등이 있다.,
개인화, 합성 데이터 활용 사례,개인화 모델 예시,DreamBooth는 프롬프트 튜닝을 응용한 사례로, 학습 토큰(Unique identifier)을 통해 영상 생성 모델을 개인화하여 특정 객체를 포함한 이미지를 생성한다.,
개인화, 합성 데이터 활용 사례,합성 데이터 활용 (데이터 효율화),합성 데이터는 실제 데이터 수집이 어렵거나 부족할 때, 실제 데이터를 모방하거나 새로 생성하여 모델 성능 개선에 사용되는 실용적인 방법론이다.,
개인화, 합성 데이터 활용 사례,합성 데이터 활용법 1 (지식 증류),Knowledge Distillation(지식 증류)은 고성능의 큰 모델(Teacher)의 지식을 모방하도록 작은 모델(Student)을 학습시켜 효율적인 모델을 만드는 방법이다.,
개인화, 합성 데이터 활용 사례,합성 데이터 활용법 2,InstructPix2Pix는 GPT-3 Fine-tuning을 사용하여 지시사항과 이미지 설명(caption)을 생성한 후, 이를 기반으로 이미지 편집 생성 모델을 학습시키는 데 사용되는 합성 데이터 활용 방법이다.,
개인화, 합성 데이터 활용 사례,AI 모델 활용 및 배포 (실습),Huggingface는 AI 관련 오픈 소스 모델과 데이터셋을 공유하는 플랫폼이며, 모델 서빙 및 배포를 지원한다.,
개인화, 합성 데이터 활용 사례,AI 모델 활용 및 배포 (실습),Gradio는 머신러닝 모델을 웹 인터페이스로 쉽게 배포할 수 있도록 돕는 오픈 소스 라이브러리이다.,