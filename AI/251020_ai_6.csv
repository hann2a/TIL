대분류,중분류,세부 내용,키워드,출처
텍스트 파운데이션 모델 살펴보기,텍스트 파운데이션 모델(거대 언어 모델)이란?,대량의 데이터를 기반으로 사전 학습된 대규모 AI 모델로, 다양한 작업에 범용적으로 활용 가능한 기초(foundation) 역할을 함,대규모 AI 모델,Foundation Model,
텍스트 파운데이션 모델 살펴보기,텍스트 파운데이션 모델(거대 언어 모델)이란?,**규모의 법칙(Scaling Law)**: 더 많은 데이터, 큰 모델, 긴 학습이 더 좋은 성능으로 이어짐,규모의 법칙,Scaling Law,
텍스트 파운데이션 모델 살펴보기,텍스트 파운데이션 모델(거대 언어 모델)이란?,**창발성(Emergent Property)**: 특정 규모를 넘어서면 모델에서 갑자기 발현되는 새로운 능력 (예: 인-컨텍스트 학습, 추론 능력),창발성,Emergent Property,In-context Learning,
텍스트 파운데이션 모델 살펴보기,텍스트 파운데이션 모델(거대 언어 모델)이란?,3가지 구성요소: **빅데이터** (인터넷 데이터 기하급수적 증가, 학습 데이터 증가 시 성능 증가),빅데이터,Big Data,
텍스트 파운데이션 모델 살펴보기,텍스트 파운데이션 모델(거대 언어 모델)이란?,3가지 구성요소: **자가 학습(Self-supervised Learning) 알고리즘** (사람의 정답 불필요, 예: 다음 토큰 예측),자가 학습,Self-supervised Learning,다음 토큰 예측,
텍스트 파운데이션 모델 살펴보기,텍스트 파운데이션 모델(거대 언어 모델)이란?),3가지 구성요소: **어텐션 기반 트랜스포머(Transformer) 모델** (더 많은 데이터를 학습하기 위한 신경망 구조),어텐션,트랜스포머,Transformer,
텍스트 파운데이션 모델 살펴보기,거대 언어 모델 예시,**폐쇄형(Closed) 거대 언어 모델**: ChatGPT (OpenAI), Claude (Anthropic), Gemini (Google) 등,폐쇄형,ChatGPT,Claude,Gemini,
텍스트 파운데이션 모델 살펴보기,거대 언어 모델 예시,폐쇄형 모델의 장점: 일반적으로 더 우수한 성능과 최신 지식, 사용 용이,성능,최신 지식,
텍스트 파운데이션 모델 살펴보기,거대 언어 모델 예시,폐쇄형 모델의 단점: 사용 시 비용 발생, 출력 정보가 제한적임,비용,정보 제한,
텍스트 파운데이션 모델 살펴보기,거대 언어 모델 예시,**개방형(Open sourced) 거대 언어 모델**: LLaMA (Meta), Gemma (Google), Qwen (Alibaba) 등,개방형,LLaMA,Gemma,
텍스트 파운데이션 모델 살펴보기,거대 언어 모델 예시,개방형 모델의 장점: 무료 다운로드 및 사용, 모델 소스 코드 공개,오픈소스,소스코드,
텍스트 파운데이션 모델 살펴보기,거대 언어 모델 예시,개방형 모델의 단점: 충분한 계산 자원(GPU) 필요, 폐쇄형 모델 대비 성능이 낮은 편,계산 자원,GPU,
거대 언어 모델의 학습,거대 언어 모델의 학습 (GPT-3),GPT-3는 거대 언어 모델의 시초로, 이전 모델 대비 10배 이상 큰 모델이며 인-컨텍스트 학습 능력이 나타나기 시작함,GPT-3,인-컨텍스트 학습,
거대 언어 모델의 학습,거대 언어 모델의 학습 (GPT-3),GPT-3 학습 방법은 다음 토큰 예측이며, 약 3000억 개 토큰(4TB)의 데이터와 150억 원 정도의 학습 비용이 추산됨,다음 토큰 예측,학습 비용,
거대 언어 모델의 학습,지시 학습 (Instruction Tuning),**정렬(Alignment) 학습**의 한 종류로, 주어진 지시에 대해 어떤 응답이 생성되어야 하는지 추가 학습 (SFT: Supervised Fine-Tuning),정렬,Alignment,SFT,
거대 언어 모델의 학습,지시 학습 (Instruction Tuning),다양한 지시 기반 입력과 응답을 통해 추가 학습(FLAN/T0)하여 학습에 보지 못한 지시에 대한 일반화 성능이 향상됨,FLAN,T0,일반화 성능,
거대 언어 모델의 학습,선호 학습 (Preference Learning),**정렬(Alignment) 학습**의 한 종류로, 다양한 응답 중 사람이 더 선호하는 응답을 생성하도록 추가 학습,선호도,Preference Learning,
거대 언어 모델의 학습,선호 학습 (Preference Learning),지시 학습의 한계(정답이 정해지지 않은 개방형 태스크)를 극복하며, **RLHF** (Reinforcement Learning from Human Feedback)가 핵심 아이디어임,RLHF,Human Feedback,
거대 언어 모델의 학습,선호 학습 (Preference Learning),InstructGPT의 학습 과정: 1. 지시 학습(SFT) $\to$ 2. 보상 모델 학습(RM) $\to$ 3. 강화 학습(PPO)을 통해 높은 보상을 주는 응답 생성,InstructGPT,SFT,RM,PPO,
거대 언어 모델의 학습,선호 학습 (Preference Learning),InstructGPT 결과: 단순 프롬프팅이나 지시 학습에 비해 발전된 지시 수행 능력, 해로운 응답(RalToxicity) 및 환각(Hallucinations)을 덜 생성,안전성,환각 감소,
거대 언어 모델의 추론,디코딩 (Decoding) 알고리즘,학습이 완료된 거대 언어 모델이 순차적인 추론을 통해 토큰별로 응답을 생성하는 방식 (Auto-regressive Generation),자동회귀 생성,Auto-regressive Generation,
거대 언어 모델의 추론,디코딩 (Decoding) 알고리즘,**① Greedy Decoding**: 가장 확률이 높은 다음 토큰을 선택 (장점: 사용 용이 / 단점: 최종 응답이 최선이 아닐 수 있음),Greedy Decoding,탐욕,
거대 언어 모델의 추론,디코딩 (Decoding) 알고리즘,**② Beam Search**: 확률이 높은 $k$개의 후보를 동시에 고려 (장점: 좋은 응답 생성 확률 증가 / 단점: 큰 계산 비용),Beam Search,빔 서치,계산 비용,
거대 언어 모델의 추론,디코딩 (Decoding) 알고리즘,**③ Sampling**: 모델의 확률을 기준으로 랜덤하게 생성 (장점: 다양한 응답 가능 / 단점: 생성 응답의 품질 불안정),Sampling,랜덤 샘플링,
거대 언어 모델의 추론,디코딩 (Decoding) 알고리즘,**④ Sampling with "Temperature" (T)**: T>1은 확률 분포를 Smooth하게 만들어 더 다양한 응답 생성, T<1은 확률 분포를 Sharp하게 만들어 기존 확률이 높은 응답에 집중,Temperature,온도,
거대 언어 모델의 추론,디코딩 (Decoding) 알고리즘,**⑤ Top-K Sampling**: 확률이 높은 K개의 토큰 중에서 랜덤 샘플링 (장점: 잡음 단어 배제, 품질 향상 / 단점: K 고정으로 문맥 따라 불균형),Top-K Sampling,K개 토큰,
거대 언어 모델의 추론,디코딩 (Decoding) 알고리즘,**⑥ Top-P Sampling (Nucleus Sampling)**: 누적 확률(P)에 집중하여 K를 자동 조절 (장점: 품질/다양성 균형, 좋은 성능),Top-P Sampling,Nucleus Sampling,누적 확률,
거대 언어 모델의 추론,프롬프트 엔지니어링,원하는 응답을 얻기 위해 모델에 주어지는 입력(프롬프트)을 설계/조정하는 기법,프롬프트,Prompt Engineering,
거대 언어 모델의 추론,프롬프트 엔지니어링,입력 프롬프트는 (1) **지시 (instruction)**와 (2) **예시 (few-shot examples)**로 구성될 수 있음,Few-shot,지시,
거대 언어 모델의 추론,프롬프트 엔지니어링,**Chain-of-Thought (CoT) 프롬프팅**: 단순 질문과 예시 대신, **추론(Reasoning) 과정**을 예시에 포함시켜 정확도를 높임,Chain-of-Thought,CoT,추론 과정,
거대 언어 모델의 추론,프롬프트 엔지니어링,CoT 결과: 거대 언어 모델의 추론 성능을 크게 증가시키며, 모델 크기가 커질수록 성능 향상이 확대됨 (창발성),추론 성능,창발성,
거대 언어 모델의 추론,프롬프트 엔지니어링,**0-shot CoT 프롬프팅**: 예시 없이 "Let's think step by step"과 같은 유인 문장을 통해 추론 성능을 향상시킴,0-shot CoT,Let's think step by step,
거대 언어 모델의 평가와 응용,거대 언어 모델의 평가,평가(Evaluation)는 구축된 시스템(코드나 앱)이 실제 잘 동작하는지 확인하는 단계로, 목표, 평가 방법, 평가 지표의 3가지 요소가 있음,Evaluation,평가 3요소,
거대 언어 모델의 평가와 응용,거대 언어 모델의 평가,정답이 정해진 태스크의 경우: 예측과 정답을 비교하여 **정확도(Accuracy)**를 측정 (예: MMLU 벤치마크),MMLU,Accuracy,
거대 언어 모델의 평가와 응용,거대 언어 모델의 평가,정답이 정해지지 않은 태스크 (예: 문서 요약) 평가 방법 #1: 사람이 임의의 정답 작성 후 예측과 비교 (ROUGE 또는 코사인 유사도 등),ROUGE,코사인 유사도,
거대 언어 모델의 평가와 응용,거대 언어 모델의 평가,정답이 정해지지 않은 태스크 평가 방법 #2: 정답과 무관하게 생성 텍스트 자체의 품질 측정 (예: **Perplexity (PPL)**),PPL,Perplexity,
거대 언어 모델의 평가와 응용,거대 언어 모델의 평가,정답이 정해지지 않은 태스크 평가 방법 #3: 생성 텍스트의 "**상대적 선호**" 평가 (**LLM-as-judge** 또는 G-Eval 활용),LLM-as-judge,G-Eval,상대적 선호,
거대 언어 모델의 평가와 응용,거대 언어 모델의 평가,LLM-as-judge 평가의 한계: **위치 편향**, **길이 편향** (길이가 긴 응답 선호), **자기 선호 편향**이 존재할 수 있음,위치 편향,길이 편향,자기 선호 편향,
거대 언어 모델의 평가와 응용,거대 언어 모델의 응용 및 한계,응용: **멀티모달 파운데이션 모델** (이미지, 비디오, 오디오 등 다양한 데이터를 이해하도록 학습),멀티모달,GPT-4o,VideoPoet,
거대 언어 모델의 평가와 응용,거대 언어 모델의 응용 및 한계,응용: **합성 데이터 생성** (Self-instruct, Alpagaus 등), 사람이 만든 데이터와 유사한 성능 달성,Self-instruct,Alpagaus,합성 데이터,
거대 언어 모델의 평가와 응용,거대 언어 모델의 응용 및 한계,한계: **환각 (Hallucination)** - 사실과 다르거나 부정확한 응답을 자신감 있게 생성,환각,Hallucination,
거대 언어 모델의 평가와 응용,거대 언어 모델의 응용 및 한계,환각 문제의 해결 방법: **검색 증강 생성(RAG)** 기능을 거대 언어 모델 서비스에 탑재하여 해결 가능,RAG,검색 증강 생성,
거대 언어 모델의 평가와 응용,거대 언어 모델의 응용 및 한계,한계: **탈옥 (Jailbreaking)** - 프롬프트 엔지니어링을 통해 모델의 정렬된 규칙을 우회할 수 있음 (예: DAN 프롬프팅),탈옥,Jailbreaking,DAN,
거대 언어 모델의 평가와 응용,거대 언어 모델의 응용 및 한계,한계: **AI 텍스트 검출** - 거대 언어 모델이 생성한 텍스트를 탐지할 수 있는 시스템이 필요함,AI 텍스트 검출,GPTZero,